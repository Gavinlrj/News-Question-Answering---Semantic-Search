{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "47603dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "file_path = 'news_dataset.csv'\n",
    "df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "\n",
    "original_columns = ['id', 'author', 'date', 'year', 'month', 'topic', 'article']\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\?', '', text)  \n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  \n",
    "    return text\n",
    "\n",
    "df['clean_article'] = df['article'].apply(lambda x: clean_text(x) if pd.notnull(x) else x)\n",
    "df = df.dropna(subset=['clean_article'])\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    words = [ps.stem(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "df['clean_article'] = df['clean_article'].apply(preprocess_text)\n",
    "\n",
    "processed_data = df[original_columns + ['clean_article']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767c7fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "156f4ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer, BertModel\n",
    "\n",
    "\n",
    "qa_model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "qa_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "\n",
    "embed_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "embed_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c412e0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def basic_coreference_resolution(text):\n",
    "    doc = nlp(text)\n",
    "    resolved_text = text\n",
    "    entities = [(ent.text, ent.start_char, ent.ead_char, ent.label_) for ent in doc.ents]\n",
    "    \n",
    "\n",
    "    pronouns = [(token.text, token.idx) for token in doc if token.pos_ == 'PRON']\n",
    "    \n",
    "    for pronoun, pronoun_idx in pronouns:\n",
    "\n",
    "        for entity_text, start, end, label in reversed(entities):\n",
    "            if pronoun.lower() == \"he\" and label == \"PERSON\":\n",
    "                resolved_text = resolved_text[:pronoun_idx] + entity_text + resolved_text[pronoun_idx + len(pronoun):]\n",
    "                break\n",
    "            elif pronoun.lower() == \"she\" and label == \"PERSON\":\n",
    "                resolved_text = resolved_text[:pronoun_idx] + entity_text + resolved_text[pronoun_idx + len(pronoun):]\n",
    "                break\n",
    "            elif pronoun.lower() == \"it\" and label in {\"ORG\", \"GPE\"}:\n",
    "                resolved_text = resolved_text[:pronoun_idx] + entity_text + resolved_text[pronoun_idx + len(pronoun):]\n",
    "                break\n",
    "            elif pronoun.lower() in {\"they\", \"them\"}:\n",
    "                resolved_text = resolved_text[:pronoun_idx] + entity_text + resolved_text[pronoun_idx + len(pronoun):]\n",
    "                break\n",
    "    \n",
    "    return resolved_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f3f958ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_sentence_embedding(sentence):\n",
    "    inputs = embed_tokenizer(sentence, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
    "    outputs = embed_model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings.detach().numpy()\n",
    "\n",
    "def find_most_relevant_sentence(question, article):\n",
    "    question_embedding = get_sentence_embedding(question)\n",
    "    sentences = article.split('. ')\n",
    "    max_similarity = -1\n",
    "    most_relevant_sentence = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_embedding = get_sentence_embedding(sentence)\n",
    "        similarity = cosine_similarity(question_embedding, sentence_embedding)\n",
    "        \n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            most_relevant_sentence = sentence\n",
    "    \n",
    "    return most_relevant_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ec29c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(article_text, question):\n",
    "    '''\n",
    "    Takes a `question` string and an `article_text` string (which contains the\n",
    "    answer), and identifies the words within the `article_text` that are the\n",
    "    answer. Prints them out.\n",
    "    '''\n",
    "\n",
    "    resolved_article_text = basic_coreference_resolution(article_text)\n",
    "    \n",
    "\n",
    "    inputs = tokenizer.encode_plus(question, resolved_article_text, return_tensors='pt', max_length=512, truncation=True)\n",
    "    \n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "    \n",
    "\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores) + 1\n",
    "\n",
    "\n",
    "    input_ids = inputs['input_ids'].tolist()[0]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    answer = tokenizer.convert_tokens_to_string(tokens[answer_start:answer_end])\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "article_id = 17574\n",
    "question = \"Who is the vice chairman of Samsung?\"\n",
    "\n",
    "\n",
    "article_text = df[df['id'] == article_id]['article'].values[0]\n",
    "\n",
    "\n",
    "answer = answer_question(article_text, question)\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e103b7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Relevant Sentence: Lee effectively runs Samsung, South Korea?s largest conglomerateLeee is the son of its chairman, Lee   who has been incapacitated with health problemsLeeHe is expected to be asked whether   donations that Samsung made to two foundations controlled by Choi   a longtime friend of the president, amounted to bribes, and what role, if anLee he played in the decision to give the money\n",
      "Answer: lee\n"
     ]
    }
   ],
   "source": [
    "def answer_question(article_text, question):\n",
    "    '''\n",
    "    Takes a `question` string and an `article_text` string (which contains the\n",
    "    answer), and identifies the words within the `article_text` that are the\n",
    "    answer. Prints them out.\n",
    "    '''\n",
    "\n",
    "    resolved_article_text = basic_coreference_resolution(article_text)\n",
    "    \n",
    "\n",
    "    most_relevant_sentence = find_most_relevant_sentence(question, resolved_article_text)\n",
    "    \n",
    "\n",
    "    inputs = qa_tokenizer.encode_plus(question, most_relevant_sentence, return_tensors='pt', max_length=512, truncation=True)\n",
    "    \n",
    "\n",
    "    outputs = qa_model(**inputs)\n",
    "    \n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "    \n",
    "\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores) + 1\n",
    "\n",
    "\n",
    "    input_ids = inputs['input_ids'].tolist()[0]\n",
    "    tokens = qa_tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    answer = qa_tokenizer.convert_tokens_to_string(tokens[answer_start:answer_end])\n",
    "\n",
    "    return most_relevant_sentence, answer\n",
    "\n",
    "\n",
    "article_id = 17574\n",
    "question = \"Who is the vice chairman of Samsung?\"\n",
    "\n",
    "\n",
    "article_text = df[df['id'] == article_id]['article'].values[0]\n",
    "\n",
    "most_relevant_sentence, answer = answer_question(article_text, question)\n",
    "print(\"Most Relevant Sentence:\", most_relevant_sentence)\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "594bbdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_f1(predicted_answer, true_answer):\n",
    "    predicted_tokens = set(predicted_answer.lower().split())\n",
    "    true_tokens = set(true_answer.lower().split())\n",
    "    \n",
    "    common_tokens = predicted_tokens.intersection(true_tokens)\n",
    "    \n",
    "    if len(common_tokens) == 0:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    precision = len(common_tokens) / len(predicted_tokens)\n",
    "    recall = len(common_tokens) / len(true_tokens)\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "120d41e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "import spacy\n",
    "\n",
    "\n",
    "qa_model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "qa_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def basic_coreference_resolution(text):\n",
    "    doc = nlp(text)\n",
    "    resolved_text = text\n",
    "    entities = [(ent.text, ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "    \n",
    "    \n",
    "    pronouns = [(token.text, token.idx) for token in doc if token.pos_ == 'PRON']\n",
    "    \n",
    "    for pronoun, pronoun_idx in pronouns:\n",
    "        \n",
    "        for entity_text, start, end, label in reversed(entities):\n",
    "            if pronoun.lower() == \"he\" and label == \"PERSON\":\n",
    "                resolved_text = resolved_text[:pronoun_idx] + entity_text + resolved_text[pronoun_idx + len(pronoun):]\n",
    "                break\n",
    "            elif pronoun.lower() == \"she\" and label == \"PERSON\":\n",
    "                resolved_text = resolved_text[:pronoun_idx] + entity_text + resolved_text[pronoun_idx + len(pronoun):]\n",
    "                break\n",
    "            elif pronoun.lower() == \"it\" and label in {\"ORG\", \"GPE\"}:\n",
    "                resolved_text = resolved_text[:pronoun_idx] + entity_text + resolved_text[pronoun_idx + len(pronoun):]\n",
    "                break\n",
    "            elif pronoun.lower() in {\"they\", \"them\"}:\n",
    "                resolved_text = resolved_text[:pronoun_idx] + entity_text + resolved_text[pronoun_idx + len(pronoun):]\n",
    "                break\n",
    "    \n",
    "    return resolved_text\n",
    "\n",
    "\n",
    "def answer_question(article_text, question):\n",
    "    \n",
    "    resolved_article_text = basic_coreference_resolution(article_text)\n",
    "    \n",
    "\n",
    "    inputs = qa_tokenizer.encode_plus(question, resolved_article_text, return_tensors='pt', max_length=512, truncation=True)\n",
    "    \n",
    "\n",
    "    outputs = qa_model(**inputs)\n",
    "    \n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "    \n",
    "\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores) + 1\n",
    "\n",
    "    input_ids = inputs['input_ids'].tolist()[0]\n",
    "    tokens = qa_tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    answer = qa_tokenizer.convert_tokens_to_string(tokens[answer_start:answer_end])\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e14bb476",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who is the vice chairman of Samsung?\n",
      "Predicted Answer: jay y . lee\n",
      "True Answer: Jay Y. Lee\n",
      "Precision: 0.5, Recall: 0.6666666666666666, F1-Score: 0.5714285714285715\n",
      "\n",
      "Average Precision: 0.5\n",
      "Average Recall: 0.6666666666666666\n",
      "Average F1-Score: 0.5714285714285715\n"
     ]
    }
   ],
   "source": [
    "def test_qa_system(test_data):\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1 = 0\n",
    "    count = 0\n",
    "    \n",
    "    for data in test_data:\n",
    "        article_id = data[\"article_id\"]\n",
    "        question = data[\"question\"]\n",
    "        true_answer = data[\"answer\"]\n",
    "        \n",
    "  \n",
    "        article_text = df[df['id'] == article_id]['article'].values[0]\n",
    "        \n",
    "        \n",
    "        predicted_answer = answer_question(article_text, question)\n",
    "        \n",
    "        # F1-score\n",
    "        precision, recall, f1 = compute_f1(predicted_answer, true_answer)\n",
    "        \n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_f1 += f1\n",
    "        count += 1\n",
    "        \n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Predicted Answer: {predicted_answer}\")\n",
    "        print(f\"True Answer: {true_answer}\")\n",
    "        print(f\"Precision: {precision}, Recall: {recall}, F1-Score: {f1}\\n\")\n",
    "    \n",
    "    avg_precision = total_precision / count if count > 0 else 0\n",
    "    avg_recall = total_recall / count if count > 0 else 0\n",
    "    avg_f1 = total_f1 / count if count > 0 else 0\n",
    "    \n",
    "    print(f\"Average Precision: {avg_precision}\")\n",
    "    print(f\"Average Recall: {avg_recall}\")\n",
    "    print(f\"Average F1-Score: {avg_f1}\")\n",
    "\n",
    "\n",
    "test_data = [\n",
    "    {\n",
    "        \"article_id\": 17574,\n",
    "        \"question\": \"Who is the vice chairman of Samsung?\",\n",
    "        \"answer\": \"Jay Y. Lee\"\n",
    "    },\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "test_qa_system(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3a91b4",
   "metadata": {},
   "source": [
    "## B. References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cff8c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "(based on https://mccormickml.com/2020/03/10/question-answering-with-a-fine-tuned-BERT/ )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
